# authors: Dora Doljanin, Filip Pavicic
# EDAMI Task4: Classification
# Our goal is to o build the best possible classifier based on the given datasets, according to the selected quality measures.
#######################################

library(gmodels) #results analysis
library(Hmisc) #results analysis
library(caret)
library(rpart) # rpart() - decision tree classifier
library(rpart.plot) 
library(e1071)
library(C50) # C5 classifer
library(randomForest)

#######################################
# Aim of the experiments
#######################################
# For the experiments in this task we will be working with the abalone dataset: https://archive.ics.uci.edu/ml/datasets/abalone

# we download the abalone dataset
download.file('http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data', 'abalone.data')
abalone  = read.table("abalone.data", header = FALSE, sep=",", na.strings= "*")
summary(abalone)

# This dataset is used for predicting the age of abalone from its physical measurements.
# It contains 4177 instances of abalones, and each of them has 8 attributes + the number of rings attribute, 
# which is used to predict the age of the abalone

# The 8 attributes, their datatype and the possible values are the following:
# Name		        Data Type	    Meas.	            Description
# -------	    --------------	-----------     -------------------
# Sex		          nominal			  M,F,I(infant)
# Length		      continuous	  mm	          Longest shell measurement
# Diameter	      continuous	  mm	          perpendicular to length
# Height		      continuous	  mm	          with meat in shell
# Whole weight	  continuous	  grams	        weight of the whole abalone
# Shucked weight  continuous	  grams	        weight of the meat
# Viscera weight	continuous	  grams	        gut weight (after bleeding)
# Shell weight	  continuous	  grams	        weight after being dried
# Rings		        integer			                +1.5 gives the age in years

# we name the attributes accordingly
colnames(abalone) <- c('Sex', 'Length','Diameter','Height','Whole', 'Shucked', 'Viscera','Shell','Rings')

# The number of rings is the value to predict in a form of a classification problem.
# There are 28 different values for the number of rings that can be found it this data set. 
# However, we will create a new attribute called age by transforming the rings attribute of each example into one of the 3 different values:
# "old", "middle", and "young", and then the attribute age will be our class attribute, thus we will have 3 different classes.
# After mapping the number of rings to the corresponding age of the abalone, we will remove the rings attribute as it will not be useful for us anymore.

abalone$Age = lapply(abalone[,'Rings'], function (x)
{
  if(x >10)  { "Old"}
  else if(x >8)  {"Middle"}
  else { "Young"}   
})
abalone$Age = unlist(abalone$Age);
abalone$Age = as.factor(abalone$Age)
abalone$Rings <- NULL
summary(abalone)

# We want to build the best possible classifier for the abalone data set, so we need to find a way to evaluate the success of the classifier.
# The criteria for assessing the quality of the classifier can be some of following quality metrics:
# accuracy, error, sensitivity, specificity, precision, recall, and F measure.
# Among all of them, we will mostly focus on the accuracy metrics as the quality measure of our classifier, since it is the metrics
# that is most often used for the validation of classifier's success.

# Abalones are used as a source of food and decorative items (e.g. jewelry, buttons, and inlay in furniture and musical instruments).
# The economic value of abalone is positively correlated with its age. Therefore, in order to determine its price, it is important
# to detect the age of the abalone accurately. This applies both for farmers and for customers.
# Thus, the potential practical application of the results generated by the constructed classifier
# could be to asses the market price of the abalone according to its age predicted based on its physical measurements.

set.seed(7777)

#creating training and test data sets (we take 70% of data for training and 30% of data for testing)
?createDataPartition
idTrainData <- unlist(createDataPartition(abalone$Age,p=0.7))

abaloneTrain <-abalone[idTrainData,]
abaloneTest <-abalone[-idTrainData,]

# we observe the class distribution in sets
prop.table(table(abaloneTrain$Age))
prop.table(table(abaloneTest$Age))
# we observe the number of instances belonging to each class in sets
table(abaloneTrain$Age)
table(abaloneTest$Age)
# we see that the classes are pretty balanced


############################################
# Experiments
############################################

################################################
# 1.  C5.0 classifier                                         
################################################

# Firstly, we use the Quinlan's C5.0 algorithm for classification of the abalones
?C5.0
# we build a model in a form of a decision tree
abalone_C50 <- C5.0(abaloneTrain[,-9], as.factor(abaloneTrain$Age)) 
summary(abalone_C50)
plot(abalone_C50)

#quality of classification for training data
abalone_c50_trainPred <- predict(abalone_C50, abaloneTrain)

?CrossTable
CrossTable(abalone_c50_trainPred, abaloneTrain$Age, prop.chisq = FALSE,prop.c = FALSE, 
           prop.r = FALSE, dnn = c('predicted class', 'actual class'))

?confusionMatrix
confusionMatrix(as.factor(abalone_c50_trainPred), as.factor(abaloneTrain$Age), mode="everything")
# we can see that the C5.0 classifier achieved an accuracy of 73.30 % on train data


#quality of classification for test data
abalone_c50_testPred <- predict(abalone_C50, abaloneTest)
CrossTable(abalone_c50_testPred, abaloneTest$Age, prop.chisq = FALSE,prop.c = FALSE, 
           prop.r = FALSE, dnn = c('predicted class', 'actual class'))

confusionMatrix(as.factor(abalone_c50_testPred), as.factor(abaloneTest$Age), mode="everything")
# we can see that the C5.0 classifier achieved an accuracy of 61.34 % on test data


# we build a model with parameter value rules = TRUE, which indicates that the tree should be decomposed into a rule-based model
abalone_C50R <- C5.0(abaloneTrain[,-9], as.factor(abaloneTrain$Age),  rules = TRUE) 
summary(abalone_C50R)

#quality of classification for test data
abalone_c50_testPred <- predict(abalone_C50R, abaloneTest)
CrossTable(abalone_c50_testPred, abaloneTest$Age, prop.chisq = FALSE,prop.c = FALSE, 
           prop.r = FALSE, dnn = c('predicted class', 'actual class'))

confusionMatrix(abalone_c50_testPred, as.factor(abaloneTest$Age), mode="everything")
# we can see that the rule-based model achieves an accuracy of 62.22 % on test data, as well

#Ensemble classifier (boosting)
#tree
abalone_C50 <- C5.0(abaloneTrain[, -20], as.factor(abaloneTrain$Age))
abalone_C50_testPred =predict(abalone_C50, abaloneTest)
confusionMatrix(abalone_C50_testPred, as.factor(abaloneTest$Age), mode="everything")

#ensemble tree
?C5.0
# we set the trials argument to 10, specifying to the algorithm that it should execute 10 boosting iterations
abalone_C50B <- C5.0(abaloneTrain[, -9], as.factor(abaloneTrain$Age),trials = 10) 
abalone_C50B_testPred =predict(abalone_C50B, abaloneTest)
confusionMatrix(abalone_C50B_testPred, as.factor(abaloneTest$Age), mode="everything")
summary(abalone_C50B)
# we can see that the boosting achieved an accuracy of 64.46 %

################################################################
# 2.  rpart - Recursive Partitioning and Regression Trees     
################################################################

?rpart
# we define the formula that links the target variable to the independent features
abaloneFormula <-  Age ~ Sex + Length + Diameter + Height + Whole + Shucked + Viscera + Shell 
# we build a tree
abalone_rpart <- rpart(abaloneFormula,  method="class", data=abaloneTrain)
print(abalone_rpart)

# CP is a complexity parameter and it serves as a penalty to control the size of the tree
# the greater the CP value, the fewer the number of splits there are
# rel error = (average_deviance_of_the_current_tree) / (average_deviance_of_the_null_tree)
# xerror value represents the relative error estimated by a 10-fold classification 
# xstd stands for the standard error of the relative error

summary(abalone_rpart)

?rpart.plot
# we plot an rpart model
rpart.plot(abalone_rpart, main="Classification for Abalone")

abalone_rpart <- rpart(abaloneFormula,  method="class", data=abaloneTrain)
print(abalone_rpart)

#training data classification - confusion matrix 
abalone_rpat_trainPred = predict(abalone_rpart,abaloneTrain,type = "class")
table(abalone_rpat_trainPred, abaloneTrain$Age)

#test data classification - confusion matrix 
abalone_rpat_testPred = predict(abalone_rpart,abaloneTest,type = "class")
table(abalone_rpat_testPred, abaloneTest$Age)

confusionMatrix(abalone_rpat_testPred, abaloneTest$Age, mode="everything")
# we can see that the accuracy on test data for the recursive partitioning is 62.94 %

#Loss matrix - a row - the actual class, a column - predicted class 
#(The loss matrix must have zeros on the diagonal and positive off-diagonal elements)
lossM=matrix(c(0,1,1,1,0,14,1,1,0), byrow=TRUE, nrow=3)
lossM
abalone_rpartLM <-  rpart(abaloneFormula,  method="class", data=abaloneTrain, parms = list(loss = lossM ))

#training data classification - confusion matrix 
abalone_rpatLM_trainPred = predict(abalone_rpartLM,abaloneTrain,type = "class")
table(abalone_rpatLM_trainPred, abaloneTrain$Age)

#test data classification - confusion matrix 
abalone_rpatLM_testPred = predict(abalone_rpartLM,abaloneTest,type = "class")
table(abalone_rpatLM_testPred, abaloneTest$Age)

#changing the values of parameters
rpControl = rpart.control(minbucket =30, maxDepth = 1);
rpTree <- rpart(abaloneFormula,  method="class", data=abaloneTrain,
                control =rpControl,
                parms = list(split = "information" ))
rpart.plot(rpTree, main="Classification for Abalone")

abalone_rpartS = predict(rpTree,abaloneTrain,type = "class")
table(abalone_rpartS, abaloneTrain$Age)

# we do the tree pruning 

# The cost complexity pruning algorithm considers the cost complexity of a tree to be a function of
# the number of leaves in the tree and the error rate of the tree (where the error rate is the
# percentage of tuples misclassified by the tree). It starts from the bottom of the tree. For
# each internal node, N, it computes the cost complexity of the subtree at N, and the cost
# complexity of the subtree at N if it were to be pruned (i.e., replaced by a leaf node). The
# two values are compared. If pruning the subtree at node N would result in a smaller cost
# complexity, then the subtree is pruned. Otherwise, it is kept.
# A pruning set of class-labeled tuples is used to estimate cost complexity. This set is
# independent of the training set used to build the unpruned tree and of any test set used
# for accuracy estimation. The algorithm generates a set of progressively pruned trees. In
# general, the smallest decision tree that minimizes the cost complexity is preferred.

#the minimal  cross-validation error 
min(abalone_rpartLM$cptable[,"xerror"])
which.min(abalone_rpartLM$cptable[,"xerror"])
rpTree.cp=abalone_rpartLM$cptable[3,"CP"]
rpTree.cp
?prune
abalone_rpartLM_Pruned<- prune(abalone_rpartLM, cp = rpTree.cp)

rpart.plot(abalone_rpartLM, main="Classification for Abalone")
rpart.plot(abalone_rpartLM_Pruned, main="Classification for Abalone - pruned")

abalone_rpartLM_Pruned <- prune(abalone_rpartLM, cp = rpTree$cptable[which.min(rpTree$cptable[,"xerror"]),"CP"])

rpart.plot(abalone_rpartLM, main="Classification for Abalone")
rpart.plot(abalone_rpartLM_Pruned, main="Classification for Abalone - pruned")

################################################################
#  3. randomForest                                             #
################################################################

?randomForest
abalone_Forest = randomForest(as.factor(Age)~., data = abaloneTrain, importance = TRUE, nodesize = 10, mtry = 4, ntree = 100 )
#nodesize = minimal number of objects in a node
#mtry - the number of randomly selected attributes for searching the best test split in nodes
#ntree -  number of trees in a forest
#importance - calculation of attriubte importance

print(abalone_Forest)
plot(abalone_Forest)

?importance
round(importance(abalone_Forest, type = 1),2)

abalone_Forest_testPred = predict (abalone_Forest, newdata = abaloneTest[-9])
confusionMatrix(abalone_Forest_testPred, as.factor(abaloneTest$Age), mode = "everything")
# we see that the accuracy equals to 63.98 %

#looking for the best values of parameters by means of K-fold validation
?trainControl
trControl <- trainControl(method = "cv", number = 10, search = "grid")

#arguments
#- method = "cv": The method used to resample the dataset. 
#- number = n: Number of folders to create
#- search = "grid": Use the search grid method. For randomized method, use "grid"

?train
tuneGrid <- expand.grid(mtry = c(1:8))
tuneGrid
abalone_Frestores_mtry <- train(Age~.,  data = abaloneTrain,
                            method = "rf",
                            metric = "Accuracy",
                            tuneGrid = tuneGrid,
                            trControl = trControl,
                            importance = TRUE,    # randomForest function parameter
                            nodesize = 10,        # randomForest function parameter
                            ntree = 250)          ## randomForest function parameter
print(abalone_Frestores_mtry)
#Accuracy was used to select the optimal model using the largest value.
#The final value used for the model was mtry = 2
#
treesModels <- list()
for (nbTree in c(5,10,25, 50, 100, 250, 500)) 
{
  abalone_F_maxtrees <- train(Age~.,  data = abaloneTrain,
                          method = "rf",
                          metric = "Accuracy",
                          tuneGrid = tuneGrid,
                          trControl = trControl,
                          importance = TRUE,
                          nodesize = 10,
                          ntree = nbTree)
  key <- toString(nbTree)
  treesModels[[key]] <- abalone_F_maxtrees
}

?resamples
results_tree <- resamples(treesModels)
summary(results_tree)

#ko?cowy model
abalone_Forest2 = randomForest(as.factor(Age)~., data = abaloneTrain, importance = TRUE, mtry = 6, ntree = 250, nodesize = 10)

print(abalone_Forest2)
plot(abalone_Forest2)
legend("top", colnames(abalone_Forest2$err.rate),col=1:5,cex=0.8,fill=1:5)


abalone_Forest2_testPred = predict (abalone_Forest, newdata = abaloneTest[-9])
confusionMatrix(abalone_Forest2_testPred, as.factor(abaloneTest$Age), mode = "everything")
# We see that the random forest model achieves an accuracy of 64.06 %. 

varImpPlot(abalone_Forest2)

################################################
#Comparision of classifiers
###############################################
abalone_rpart <- rpart(Age~., data=abaloneTrain)
abalone_rpart_testPred = predict(abalone_rpart, abaloneTest, type = "class")

classifier = c('C50', 'rpart',  'rForest')
accuracy = c( mean(abalone_c50_testPred == abaloneTest$Age), 
                mean(abalone_rpart_testPred == abaloneTest$Age),
                mean(abalone_Forest2_testPred == abaloneTest$Age))

res <- data.frame(classifier, accuracy)
View(res)

#################################################
# Conclusion
#################################################
# Based on the conducted experiments,  we search for the best possible classifier. Since we have chosen the accuracy metrics as our 
# criteria for assessing the quality of the classifier, we observe and compare the accuracy of all three models C50, rpart, and rForest.
# Accuracy on abalone test data:
# The results show that the C5.0 classifier achieves an accuracy of 62.22 %, 
# the Recursive Partitioning model achieves an accuracy of 62.93 %,
# while the Random Forest classifier achieves an accuracy of 64.05 %.

# We see that the rForest classifier achieves the highest accuracy out of the three classifiers. Thus, according to the results of the experiments,
# we conclude that the random forest classifier is the best classifier based on the abalone dataset.